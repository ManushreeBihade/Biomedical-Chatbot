# ğŸ§¬ Biomedical Multi-LLM Chatbot

A modular biomedical chatbot built using **FastAPI (backend)** and **Streamlit (frontend)** with support for multiple LLM providers.

Supports:

* âœ… Groq (Llama 3.1)
* âœ… Google Gemini
* âœ… Memory toggle (ON/OFF)
* âœ… Automatic context summarization
* âœ… Clean layered backend architecture

## ğŸ— Architecture

User (Streamlit UI)
        â†“
FastAPI Backend
        â†“
Service Layer (LLM Logic)
        â†“
Groq / Gemini APIs

# Backend is structured into:

* `main.py` â†’ App entry point
* `routes.py` â†’ API endpoints
* `schemas.py` â†’ Request/response validation
* `config.py` â†’ Environment configuration
* `llm_service.py` â†’ Core AI logic

## ğŸš€ Run Locally

### 1ï¸âƒ£ Clone Repository

git clone https://github.com/ManushreeBihade/Biomedical-Chatbot.git
cd Biomedical-Chatbot

### 2ï¸âƒ£ Create Virtual Environment

**Windows**

python -m venv venv
venv\Scripts\activate

**Mac/Linux**

python3 -m venv venv
source venv/bin/activate

### 3ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

### 4ï¸âƒ£ Add API Keys

Create a `.env` file in the root directory:
GROQ_API_KEY=your_groq_api_key
GEMINI_API_KEY=your_gemini_api_key

### 5ï¸âƒ£ Run Backend (FastAPI)

uvicorn backend.main:app --reload

Backend runs at:
http://127.0.0.1:8000

API Docs:
http://127.0.0.1:8000/docs

### 6ï¸âƒ£ Run Frontend (Streamlit)

Open a new terminal:
streamlit run frontend/app.py

The UI will open automatically in your browser.

## âš™ Features

* ğŸ” Multi-LLM provider toggle
* ğŸ§  Optional conversational memory
* ğŸ“‰ Automatic summarization when chat context exceeds threshold
* ğŸ§© Layered backend design (production-ready structure)
* ğŸ” Secure API key handling via environment variables

## ğŸ“Œ Notes

* Run backend **before** frontend.
* Memory summarization triggers automatically when context exceeds configured limit.
* Designed for biomedical domain queries only.